{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe CRISP-DM\n",
    "\n",
    "# Business understanding – What does the business need?\n",
    "# Data understanding – What data do we have / need? Is it clean?\n",
    "# Data preparation – How do we organize the data for modeling?\n",
    "# Modeling – What modeling techniques should we apply?\n",
    "# Evaluation – Which model best meets the business objectives?\n",
    "# Deployment – How do stakeholders access the results?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Business understanding – What does the business need?\n",
    "# Determine business objectives: You should first “thoroughly understand, from a business perspective, what the customer really wants to accomplish.” (CRISP-DM Guide) and then define business success criteria.\n",
    "# Assess situation: Determine resources availability, project requirements, assess risks and contingencies, and conduct a cost-benefit analysis.\n",
    "# Determine data mining goals: In addition to defining the business objectives, you should also define what success looks like from a technical data mining perspective.\n",
    "# Produce project plan: Select technologies and tools and define detailed plans for each project phase.\n",
    "\n",
    "# II. Data Understanding\n",
    "# Collect initial data: Acquire the necessary data and (if necessary) load it into your analysis tool.\n",
    "# Describe data: Examine the data and document its surface properties like data format, number of records, or field identities.\n",
    "# Explore data: Dig deeper into the data. Query it, visualize it, and identify relationships among the data.\n",
    "# Verify data quality: How clean/dirty is the data? Document any quality issues.\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# Select data: Determine which data sets will be used and document reasons for inclusion/exclusion.\n",
    "# Clean data: Often this is the lengthiest task. Without it, you’ll likely fall victim to garbage-in, garbage-out. A common practice during this task is to correct, impute, or remove erroneous values.\n",
    "# Construct data: Derive new attributes that will be helpful. For example, derive someone’s body mass index from height and weight fields.\n",
    "# Integrate data: Create new data sets by combining data from multiple sources.\n",
    "# Format data: Re-format data as necessary. For example, you might convert string values that store numbers to numeric values so that you can perform mathematical operations.\n",
    "\n",
    "# Modeling\n",
    "# Select modeling techniques: Determine which algorithms to try (e.g. regression, neural net).\n",
    "# Generate test design: Pending your modeling approach, you might need to split the data into training, test, and validation sets.\n",
    "# Build model: As glamorous as this might sound, this might just be executing a few lines of code like “reg = LinearRegression().fit(X, y)”.\n",
    "# Assess model: Generally, multiple models are competing against each other, and the data scientist needs to interpret the model results based on domain knowledge, the pre-defined success criteria, and the test design.\n",
    "\n",
    "#Evaluation\n",
    "# Evaluate results: Do the models meet the business success criteria? Which one(s) should we approve for the business?\n",
    "# Review process: Review the work accomplished. Was anything overlooked? Were all steps properly executed? Summarize findings and correct anything if needed.\n",
    "# Determine next steps: Based on the previous three tasks, determine whether to proceed to deployment, iterate further, or initiate new projects.\n",
    "\n",
    "#Deployment\n",
    "# Plan deployment: Develop and document a plan for deploying the model.\n",
    "# Plan monitoring and maintenance: Develop a thorough monitoring and maintenance plan to avoid issues during the operational phase (or post-project phase) of a model.\n",
    "# Produce final report: The project team documents a summary of the project which might include a final presentation of data mining results.\n",
    "# Review project: Conduct a project retrospective about what went well, what could have been better, and how to improve in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all experiments, you should compare the technical performance on both red and white\n",
    "# wine datasets.\n",
    "# You should perform the following experiments:\n",
    "# • Use supervised and unsupervised methods (see following sections);\n",
    "# • Randomly remove 10%, 20%, and 30% of the values of the features of each dataset and\n",
    "# explore two different strategies to handle missing values;\n",
    "# • Experiment with data normalization, data discretization, and data reduction. Apply\n",
    "# these steps to the original, unchanged, dataset.\n",
    "# Don’t forget to visually explore your data, namely presenting correlations between pairs\n",
    "# of features.\n",
    "# The technical evaluation should include different metrics and means to better under-\n",
    "# stand the errors of the supervised machine learning approaches. The assessment of the\n",
    "# unsupervised machine learning approaches should compare the resulting clusters to clus-\n",
    "# ters based on the quality score.\n",
    "\n",
    "file_white_wine = 'winequality-white.csv'\n",
    "file_red_wine = 'winequality-red.csv'\n",
    "\n",
    "# Load the data\n",
    "white_wine = pd.read_csv(file_white_wine, sep=';')\n",
    "red_wine = pd.read_csv(file_red_wine, sep=';')\n",
    "\n",
    "# Print the first 5 rows of the data\n",
    "# print(white_wine.head())\n",
    "# print(red_wine.head())\n",
    "\n",
    "\n",
    "# • Randomly remove 10%, 20%, and 30% of the values of the features of each dataset and\n",
    "\n",
    "def remove_values(data, percentage):\n",
    "    # get dimensions of df\n",
    "    data = data.copy()\n",
    "    nrows, ncols = len(data.index), len(data.columns)         \n",
    "\n",
    "    volume = nrows * ncols                    # total number of entries in df\n",
    "    volume_to_be_nan = int(volume * percentage)      # number of entries to turn to NaN (10 %)\n",
    "\n",
    "    # randomly generate index locations for the new NaNs\n",
    "    indices = np.random.randint(volume, size=volume_to_be_nan)\n",
    "    row_indices = indices % nrows\n",
    "    col_indices = (indices / nrows).astype(int)\n",
    "\n",
    "    # assign NaN to each of the indices in df\n",
    "    for ri, ci in zip(row_indices, col_indices):\n",
    "        data.iloc[ri, ci] = np.nan\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# print(white_wine_10.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def initialize_data(white_wine, red_wine, process_function):\n",
    "    y_white_wine_10 = white_wine['quality']\n",
    "    x_white_wine_10 = remove_values(white_wine.drop('quality', axis=1), 0.1)\n",
    "    full_white_wine_10 = x_white_wine_10\n",
    "    full_white_wine_10['quality'] = y_white_wine_10\n",
    "    full_white_wine_10 = process_function(full_white_wine_10)\n",
    "    y_white_wine_10 = full_white_wine_10['quality']\n",
    "    x_white_wine_10 = full_white_wine_10.drop('quality', axis=1)\n",
    "    \n",
    "\n",
    "    y_red_wine_10 = red_wine['quality']\n",
    "    x_red_wine_10 = remove_values(red_wine.drop('quality', axis=1), 0.1)\n",
    "    full_red_wine_10 = x_red_wine_10\n",
    "    full_red_wine_10['quality'] = y_red_wine_10\n",
    "    full_red_wine_10 = process_function(full_red_wine_10)\n",
    "    y_red_wine_10 = full_red_wine_10['quality']\n",
    "    x_red_wine_10 = full_red_wine_10.drop('quality', axis=1)\n",
    "\n",
    "\n",
    "    y_white_wine_20 = white_wine['quality']\n",
    "    x_white_wine_20 = remove_values(white_wine.drop('quality', axis=1), 0.1)\n",
    "    full_white_wine_20 = x_white_wine_20\n",
    "    full_white_wine_20['quality'] = y_white_wine_20\n",
    "    full_white_wine_20 = process_function(full_white_wine_20)\n",
    "    y_white_wine_20 = full_white_wine_20['quality']\n",
    "    x_white_wine_20 = full_white_wine_20.drop('quality', axis=1)\n",
    "\n",
    "    y_red_wine_20 = red_wine['quality']\n",
    "    x_red_wine_20 = remove_values(red_wine.drop('quality', axis=1), 0.1)\n",
    "    full_red_wine_20 = x_red_wine_20\n",
    "    full_red_wine_20['quality'] = y_red_wine_20\n",
    "    full_red_wine_20 = process_function(full_red_wine_20)\n",
    "    y_red_wine_20 = full_red_wine_20['quality']\n",
    "    x_red_wine_20 = full_red_wine_20.drop('quality', axis=1)\n",
    "\n",
    "\n",
    "    y_white_wine_30 = white_wine['quality']\n",
    "    x_white_wine_30 = remove_values(white_wine.drop('quality', axis=1), 0.1)\n",
    "    full_white_wine_30 = x_white_wine_30\n",
    "    full_white_wine_30['quality'] = y_white_wine_30\n",
    "    full_white_wine_30 = process_function(full_white_wine_30)\n",
    "    y_white_wine_30 = full_white_wine_30['quality']\n",
    "    x_white_wine_30 = full_white_wine_30.drop('quality', axis=1)\n",
    "\n",
    "    y_red_wine_30 = red_wine['quality']\n",
    "    x_red_wine_30 = remove_values(red_wine.drop('quality', axis=1), 0.1)\n",
    "    full_red_wine_30 = x_red_wine_30\n",
    "    full_red_wine_30['quality'] = y_red_wine_30\n",
    "    full_red_wine_30 = process_function(full_red_wine_30)\n",
    "    y_red_wine_30 = full_red_wine_30['quality']\n",
    "\n",
    "\n",
    "    # print(\"white wine 10% removed nullable values\")\n",
    "    # print(full_white_wine_10.isnull().sum())\n",
    "    # print(\"red wine 10% removed nullable values\")\n",
    "    # print(full_red_wine_10.isnull().sum())\n",
    "    # print(\"white wine 20% removed nullable values\")\n",
    "    # print(full_white_wine_20.isnull().sum())\n",
    "    # print(\"red wine 20% removed nullable values\")\n",
    "    # print(full_red_wine_20.isnull().sum())\n",
    "    # print(\"white wine 30% removed nullable values\")\n",
    "    # print(full_white_wine_30.isnull().sum())\n",
    "    # print(\"red wine 30% removed nullable values\")\n",
    "    # print(full_red_wine_30.isnull().sum())\n",
    "\n",
    "    return x_white_wine_10, y_white_wine_10, x_red_wine_10, y_red_wine_10, x_white_wine_20, y_white_wine_20, x_red_wine_20, y_red_wine_20, x_white_wine_30, y_white_wine_30, x_red_wine_30, y_red_wine_30\n",
    "# print(full_red_wine_10.info())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Learning Algorithms\n",
    "# Experiment with the following supervised learning algorithms and comment the results,\n",
    "# based on your knowledge of how they work.\n",
    "# 1. Decision Trees;\n",
    "# 2. Multi-layer perceptron;\n",
    "\n",
    "# 1. Decision Trees\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def runDecisonTreeClassifier(x_train, y_train, x_test, y_test):\n",
    "    # Create a decision tree classfifer object\n",
    "\n",
    "    param_grid = { 'criterion':['gini','entropy'],'max_depth': np.arange(3, 15)}\n",
    "\n",
    "    classfifer = DecisionTreeClassifier()\n",
    "    dtree_gscv = GridSearchCV(classfifer, param_grid)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    dtree_gscv.fit(x_train, y_train)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(dtree_gscv.best_params_)\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = dtree_gscv.predict(x_test)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = dtree_gscv.score(x_test, y_test)\n",
    "    print(\"Decision Tree Accuracy: \", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# random forest\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def runDecisonRandomForestClassifier(x_train, y_train, x_test, y_test):\n",
    "    # Create a random forest classfifer object\n",
    "\n",
    "    # param_grid = { 'criterion':['gini','entropy'],'max_depth': np.arange(3, 15)}\n",
    "\n",
    "    classfifer = RandomForestClassifier(criterion='entropy', max_depth=13)\n",
    "    # dtree_gscv = GridSearchCV(classfifer, param_grid)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    classfifer.fit(x_train, y_train)\n",
    "    # print(\"Best parameters set found on development set:\")\n",
    "    # {'criterion': 'entropy', 'max_depth': 13}\n",
    "\n",
    "    # print()\n",
    "    # print(dtree_gscv.best_params_)\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = classfifer.predict(x_test)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = classfifer.score(x_test, y_test)\n",
    "    print(\"Random Forest Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Multi-layer perceptron;\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def runMultiLayerPerceptron(x_train, y_train, x_test, y_test):\n",
    "    # Create a MLP classifier object\n",
    "    param_grid = [\n",
    "            {\n",
    "                'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "                'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "                'hidden_layer_sizes': [\n",
    "                (1,),(2,),(3,),(4,), (5,), (6,), (7,), (8,), (9,), (10,),\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    \n",
    "    classifier = MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', solver = 'lbfgs', hidden_layer_sizes=(4,)) \n",
    "    # GridSearchCV(MLPClassifier(), param_grid, cv=3,\n",
    "    #                         scoring='accuracy')\n",
    "    classifier.fit(x_train,y_train)\n",
    "\n",
    "    \n",
    "    # print(\"Best parameters set found on development set:\")\n",
    "    # print(classifier.best_params_)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = classifier.predict(x_test)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = classifier.score(x_test, y_test)\n",
    "    print(\"Multi-layer Perceptron Accuracy: \", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. k-NN.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def runKnnClassifier(x_train, y_train, x_test, y_test):\n",
    "    # Create a MLP Classifier object\n",
    "    classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = classifier.predict(x_test)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = classifier.score(x_test, y_test)\n",
    "    print(\"k-NN Accuracy: \", accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised learning algorithms\n",
    "# Experiment with the following unsupervised learning algorithms and comment the results,\n",
    "# based on your knowledge of how they work.\n",
    "# 1. k-Means;\n",
    "# 2. DBScan;\n",
    "# 3. Agglomerative hierarchical clustering.\n",
    "\n",
    "# 1. k-Means\n",
    "\n",
    "\n",
    "\n",
    "#import libraries\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "    #evaluate model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def findBestK(x_train, x_test):\n",
    "    wcss = []\n",
    "    for i in range(1,11):\n",
    "        kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300, n_init=12, random_state=0)\n",
    "        kmeans.fit(x_train)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    f3, ax = plt.subplots(figsize=(8, 6))\n",
    "    plt.plot(range(1,11),wcss)\n",
    "    plt.title('The Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()\n",
    "    # To determine the optimal number of clusters, \n",
    "    # we have to select the value of k at the “elbow” ie the point after which the distortion/inertia start decreasing in a linear fashion. \n",
    "    # Thus for the given data, we conclude that the optimal number of clusters for the data is 2. \n",
    "\n",
    "    #Applying kmeans to the dataset, set k=2\n",
    "    kmeans = KMeans(n_clusters = 2)\n",
    "    start_time = time.time()\n",
    "    clusters = kmeans.fit_predict(x_test)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    labels = kmeans.labels_\n",
    "    # print(labels)\n",
    "    #2D plot\n",
    "    x_test = x_test.values\n",
    "    colors = 'rgbkcmy'\n",
    "    for i in np.unique(clusters):\n",
    "        plt.scatter(x_test[clusters==i, 0],\n",
    "                x_test[clusters==i, 1],\n",
    "                color=colors[i], label='Cluster' + str(i+1))\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "    silhouette_score = metrics.silhouette_score(x_test, labels, metric='euclidean')\n",
    "    print(\"Silhouette score: \", silhouette_score)\n",
    "    # TODO  PCA (principle component analysis).\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train_white_10, x_test_white_10, y_train_white_10, y_test_white_10 = train_test_split(x_white_wine_10, y_white_wine_10, test_size=0.2, random_state=0)\n",
    "\n",
    "# # print(white_wine_10.columns.values, 0.2)\n",
    "# findBestK(x_train_white_10, x_test_white_10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DBScan\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "def runDBScanClustering(x_train, y_train):\n",
    "\n",
    "    # Compute DBSCAN using Iris dataset\n",
    "    db = DBSCAN(eps=0.05, min_samples=10).fit(x_train)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    # Plot result\n",
    "    # import matplotlib.pyplot as plt\n",
    "\n",
    "    # # Black removed and is used for noise instead.\n",
    "    # unique_labels = set(labels)\n",
    "    # colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    # for k, col in zip(unique_labels, colors):\n",
    "    #     if k == -1:\n",
    "    #         # Black used for noise.\n",
    "    #         col = 'k'\n",
    "\n",
    "    #     class_member_mask = (labels == k)\n",
    "\n",
    "    #     xy = X[class_member_mask & core_samples_mask]\n",
    "    #     plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "    #             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    #     xy = X[class_member_mask & ~core_samples_mask]\n",
    "    #     plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "    #             markeredgecolor='k', markersize=6)\n",
    "\n",
    "    # plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "    # plt.show()\n",
    "    # Todo: Algum metodo para definir a accuracy dos clusters em relacao ao dataset original.\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train_white_10, x_test_white_10, y_train_white_10, y_test_white_10 = train_test_split(x_white_wine_10, y_white_wine_10, test_size=0.2, random_state=0)\n",
    "\n",
    "# runDBScanClustering(x_train_white_10, y_train_white_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Agglomerative hierarchical clustering.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def runAgglomerativeHierarchicalClustering(x_train, y_train, x_test, y_test):\n",
    "    # Create a Agglomerative hierarchical classifier object\n",
    "    classifier = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    classifier.fit(x_train)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = classifier.predict(x_test)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = classifier.score(x_test, y_test)\n",
    "    print(\"Agglomerative Hierarchical Clustering Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For white wine quality 10% of data\n",
      "Supervised Learning Algorithms\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 3}\n",
      "Decision Tree Accuracy:  0.4968944099378882\n",
      "Multi-layer Perceptron Accuracy:  0.45962732919254656\n",
      "k-NN Accuracy:  0.40683229813664595\n",
      "Random Forest Accuracy:  0.6055900621118012\n",
      "Unsupervised Learning Algorithms\n",
      "For white wine quality 20% of data\n",
      "Supervised Learning Algorithms\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 4}\n",
      "Decision Tree Accuracy:  0.47720364741641336\n",
      "Multi-layer Perceptron Accuracy:  0.47112462006079026\n",
      "k-NN Accuracy:  0.4376899696048632\n",
      "Random Forest Accuracy:  0.6024844720496895\n",
      "Unsupervised Learning Algorithms\n",
      "For white wine quality 30% of data\n",
      "Supervised Learning Algorithms\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 3}\n",
      "Decision Tree Accuracy:  0.4723926380368098\n",
      "Multi-layer Perceptron Accuracy:  0.44785276073619634\n",
      "k-NN Accuracy:  0.4447852760736196\n",
      "Random Forest Accuracy:  0.6149068322981367\n",
      "Unsupervised Learning Algorithms\n"
     ]
    }
   ],
   "source": [
    "# Supervised learning algorithms\n",
    "\n",
    "#test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "def runPredictions(x_white_wine_10, y_white_wine_10, x_white_wine_20, y_white_wine_20, x_white_wine_30, y_white_wine_30):\n",
    "\n",
    "\n",
    "\n",
    "    x_train_white_10, x_test_white_10, y_train_white_10, y_test_white_10 = train_test_split(x_white_wine_10, y_white_wine_10, test_size=0.2, random_state=0)\n",
    "\n",
    "    print(\"For white wine quality 10% of data\")\n",
    "\n",
    "    print(\"Supervised Learning Algorithms\")\n",
    "    runDecisonTreeClassifier(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "    runMultiLayerPerceptron(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "    runKnnClassifier(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "    runDecisonRandomForestClassifier(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "\n",
    "    print(\"Unsupervised Learning Algorithms\")\n",
    "    # runKMeans(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "    # runDBScan(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "    # runAgglomerativeHierarchicalClustering(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "\n",
    "\n",
    "    x_train_white_20, x_test_white_20, y_train_white_20, y_test_white_20 = train_test_split(x_white_wine_20, y_white_wine_20, test_size=0.2, random_state=0)\n",
    "\n",
    "    print(\"For white wine quality 20% of data\")\n",
    "\n",
    "    print(\"Supervised Learning Algorithms\")\n",
    "    runDecisonTreeClassifier(x_train_white_20, y_train_white_20, x_test_white_20, y_test_white_20)\n",
    "    runMultiLayerPerceptron(x_train_white_20, y_train_white_20, x_test_white_20, y_test_white_20)\n",
    "    runKnnClassifier(x_train_white_20, y_train_white_20, x_test_white_20, y_test_white_20)\n",
    "    runDecisonRandomForestClassifier(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "\n",
    "    print(\"Unsupervised Learning Algorithms\")\n",
    "    # runKMeans(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "    # runDBScan(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "    # runAgglomerativeHierarchicalClustering(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "\n",
    "\n",
    "    x_train_white_30, x_test_white_30, y_train_white_30, y_test_white_30 = train_test_split(x_white_wine_30, y_white_wine_30, test_size=0.2, random_state=0)\n",
    "\n",
    "    print(\"For white wine quality 30% of data\")\n",
    "\n",
    "    print(\"Supervised Learning Algorithms\")\n",
    "    runDecisonTreeClassifier(x_train_white_30, y_train_white_30, x_test_white_30, y_test_white_30)\n",
    "    runMultiLayerPerceptron(x_train_white_30, y_train_white_30, x_test_white_30, y_test_white_30)\n",
    "    runKnnClassifier(x_train_white_30, y_train_white_30, x_test_white_30, y_test_white_30)\n",
    "    runDecisonRandomForestClassifier(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "\n",
    "    print(\"Unsupervised Learning Algorithms\")\n",
    "    # runKMeans(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "    # runDBScan(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "    # runAgglomerativeHierarchicalClustering(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "\n",
    "\n",
    "\n",
    "def drop_null_values(df):\n",
    "    return df.copy().dropna()\n",
    "\n",
    "\n",
    "x_white_wine_10, y_white_wine_10, x_red_wine_10, y_red_wine_10, x_white_wine_20, y_white_wine_20, x_red_wine_20, y_red_wine_20, x_white_wine_30, y_white_wine_30, x_red_wine_30, y_red_wine_30 = initialize_data(white_wine, red_wine, drop_null_values)\n",
    "\n",
    "\n",
    "runPredictions(x_white_wine_10, y_white_wine_10, x_white_wine_20, y_white_wine_20, x_white_wine_30, y_white_wine_30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For white wine quality 10% of data\n",
      "Supervised Learning Algorithms\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 5}\n",
      "Decision Tree Accuracy:  0.4857142857142857\n",
      "Multi-layer Perceptron Accuracy:  0.48673469387755103\n",
      "k-NN Accuracy:  0.5040816326530613\n",
      "Random Forest Accuracy:  0.5918367346938775\n",
      "Unsupervised Learning Algorithms\n",
      "For white wine quality 20% of data\n",
      "Supervised Learning Algorithms\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 5}\n",
      "Decision Tree Accuracy:  0.4959183673469388\n",
      "Multi-layer Perceptron Accuracy:  0.4928571428571429\n",
      "k-NN Accuracy:  0.5183673469387755\n",
      "Random Forest Accuracy:  0.6020408163265306\n",
      "Unsupervised Learning Algorithms\n",
      "For white wine quality 30% of data\n",
      "Supervised Learning Algorithms\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 10}\n",
      "Decision Tree Accuracy:  0.5051020408163265\n",
      "Multi-layer Perceptron Accuracy:  0.48775510204081635\n",
      "k-NN Accuracy:  0.5306122448979592\n",
      "Random Forest Accuracy:  0.613265306122449\n",
      "Unsupervised Learning Algorithms\n"
     ]
    }
   ],
   "source": [
    "#lets do some preprocessing for the red wine dataset\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def preprocess_min_max_scaller(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    y = df['quality']\n",
    "    df.drop(['quality'], axis=1, inplace=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    df_transformed = scaler.fit_transform(df)\n",
    "    df_normalized = pd.DataFrame(df_transformed, columns=list(df.columns.values))\n",
    "    df_normalized['quality'] = y\n",
    "    return df_normalized\n",
    "\n",
    "def get_correlation_matrix(df):\n",
    "\n",
    "    corr_mat=df.corr()\n",
    "    #check for highly correlated values to be removed\n",
    "    target = 'quality'\n",
    "    candidates = corr_mat.index[\n",
    "        (corr_mat[target] > 0.5) | (corr_mat[target] < -0.5)\n",
    "    ].values\n",
    "    candidates = candidates[candidates != target]\n",
    "    print('Correlated to', target, ': ', candidates)\n",
    "\n",
    "\n",
    "    # lets see the correlation between eachother by using heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap=\"Reds\", mask=mask, linewidth=0.5)\n",
    "\n",
    "\n",
    "\n",
    "x_white_wine_10, y_white_wine_10, x_red_wine_10, y_red_wine_10, x_white_wine_20, y_white_wine_20, x_red_wine_20, y_red_wine_20, x_white_wine_30, y_white_wine_30, x_red_wine_30, y_red_wine_30 = initialize_data(white_wine, red_wine, preprocess_min_max_scaller)\n",
    "\n",
    "\n",
    "runPredictions(x_white_wine_10, y_white_wine_10, x_white_wine_20, y_white_wine_20, x_white_wine_30, y_white_wine_30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For white wine quality 10% of data\n",
      "Supervised Learning Algorithms\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'entropy', 'max_depth': 6}\n",
      "Decision Tree Accuracy:  0.575\n",
      "Multi-layer Perceptron Accuracy:  0.578125\n",
      "k-NN Accuracy:  0.553125\n",
      "Random Forest Accuracy:  0.696875\n",
      "Unsupervised Learning Algorithms\n",
      "For white wine quality 20% of data\n",
      "Supervised Learning Algorithms\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'entropy', 'max_depth': 5}\n",
      "Decision Tree Accuracy:  0.625\n",
      "Multi-layer Perceptron Accuracy:  0.56875\n",
      "k-NN Accuracy:  0.590625\n",
      "Random Forest Accuracy:  0.6875\n",
      "Unsupervised Learning Algorithms\n",
      "For white wine quality 30% of data\n",
      "Supervised Learning Algorithms\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_142319/1679970689.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# runPredictions(x_white_wine_10, y_white_wine_10, x_white_wine_20, y_white_wine_20, x_white_wine_30, y_white_wine_30)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mrunPredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_red_wine_10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_red_wine_10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_red_wine_20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_red_wine_20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_red_wine_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_red_wine_30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_142319/3484840319.py\u001b[0m in \u001b[0;36mrunPredictions\u001b[0;34m(x_white_wine_10, y_white_wine_10, x_white_wine_20, y_white_wine_20, x_white_wine_30, y_white_wine_30)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Supervised Learning Algorithms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mrunDecisonTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_white_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_white_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_white_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_white_30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mrunMultiLayerPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_white_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_white_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_white_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_white_30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mrunKnnClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_white_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_white_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_white_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_white_30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_142319/2666952015.py\u001b[0m in \u001b[0;36mrunDecisonTreeClassifier\u001b[0;34m(x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Train the model using the training sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdtree_gscv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best parameters set found on development set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \"\"\"\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    938\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mcheck_X_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0;31m# :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m         ):\n\u001b[1;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"infinity\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN, infinity\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# Lets try PCA and see how it goes better on this data¶\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_pca(df):\n",
    "    df = df.copy()\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    y_df = df['quality']\n",
    "\n",
    "    scalar = StandardScaler()\n",
    "    x_df = pd.DataFrame(scalar.fit_transform(df.drop('quality', axis=1)), columns=list(df.drop('quality', axis=1).columns.values))\n",
    "    # df_scaled.head()\n",
    "\n",
    "\n",
    "    # pca_df = PCA(n_components=.95)\n",
    "    # df_pca = pd.DataFrame(pca_df.fit_transform(df))\n",
    "    # x_df = df_pca \n",
    "\n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # pd.DataFrame(pca_df.explained_variance_ratio_).plot.bar()\n",
    "    # plt.legend('')\n",
    "    # plt.xlabel('Principal Components')\n",
    "    # plt.ylabel('Explained Varience');\n",
    "\n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # pd.DataFrame(pca_df.explained_variance_ratio_).plot.bar()\n",
    "    # plt.legend('')\n",
    "    # plt.xlabel('Principal Components')\n",
    "    # plt.ylabel('Explained Varience');\n",
    "\n",
    "    # we can see that 5 pcs are explaining 80% of target variable\n",
    "\n",
    "    x_df['quality'] = y_df\n",
    "\n",
    "    return x_df\n",
    "\n",
    "\n",
    "\n",
    "x_white_wine_10, y_white_wine_10, x_red_wine_10, y_red_wine_10, x_white_wine_20, y_white_wine_20, x_red_wine_20, y_red_wine_20, x_white_wine_30, y_white_wine_30, x_red_wine_30, y_red_wine_30 = initialize_data(white_wine, red_wine, apply_pca)\n",
    "\n",
    "\n",
    "# runPredictions(x_white_wine_10, y_white_wine_10, x_white_wine_20, y_white_wine_20, x_white_wine_30, y_white_wine_30)\n",
    "\n",
    "runPredictions(x_red_wine_10, y_red_wine_10, x_red_wine_20, y_red_wine_20, x_red_wine_30, y_red_wine_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4898, 9)\n",
      "(4898,)\n",
      "For white wine quality 10% of data\n",
      "Supervised Learning Algorithms\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'entropy', 'max_depth': 14}\n",
      "Decision Tree Accuracy:  0.5785714285714286\n",
      "Multi-layer Perceptron Accuracy:  0.503061224489796\n",
      "k-NN Accuracy:  0.5438775510204081\n",
      "Unsupervised Learning Algorithms\n",
      "For white wine quality 20% of data\n",
      "Supervised Learning Algorithms\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 4}\n",
      "Decision Tree Accuracy:  0.44387755102040816\n",
      "Multi-layer Perceptron Accuracy:  0.5204081632653061\n",
      "k-NN Accuracy:  0.4744897959183674\n",
      "Unsupervised Learning Algorithms\n",
      "For white wine quality 30% of data\n",
      "Supervised Learning Algorithms\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 11}\n",
      "Decision Tree Accuracy:  0.5680272108843537\n",
      "Multi-layer Perceptron Accuracy:  0.518140589569161\n",
      "k-NN Accuracy:  0.5476190476190477\n",
      "Unsupervised Learning Algorithms\n"
     ]
    }
   ],
   "source": [
    "# Supervised learning algorithms\n",
    "\n",
    "#test train split\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = random.randint(0,100)\n",
    "\n",
    "white_wine_10 = pca_analysis_white_wine.sample(frac=1, random_state=random_state)\n",
    "white_wine_20 = pca_analysis_white_wine.sample(frac=0.2, random_state=random_state)\n",
    "white_wine_30 = pca_analysis_white_wine.sample(frac=0.9, random_state=random_state)\n",
    "\n",
    "red_wine_10 = pca_analysis_red_wine.sample(frac=1, random_state=random_state)\n",
    "red_wine_20 = pca_analysis_red_wine.sample(frac=0.2, random_state=random_state)\n",
    "red_wine_30 = pca_analysis_red_wine.sample(frac=0.9, random_state=random_state)\n",
    "\n",
    "\n",
    "y_white_wine_10 = white_wine_10['quality']\n",
    "x_white_wine_10 = white_wine_10.drop('quality', axis=1)\n",
    "\n",
    "print(x_white_wine_10.shape)\n",
    "print(y_white_wine_10.shape)\n",
    "\n",
    "y_white_wine_20 = white_wine_20['quality']\n",
    "x_white_wine_20 = white_wine_20.drop('quality', axis=1)\n",
    "\n",
    "y_white_wine_30 = white_wine_30['quality']\n",
    "x_white_wine_30 = white_wine_30.drop('quality', axis=1)\n",
    "\n",
    "y_red_wine_10 = red_wine_10['quality']\n",
    "x_red_wine_10 = red_wine_10.drop('quality', axis=1)\n",
    "\n",
    "y_red_wine_20 = red_wine_20['quality']\n",
    "x_red_wine_20 = red_wine_20.drop('quality', axis=1)\n",
    "\n",
    "y_red_wine_30 = red_wine_30['quality']\n",
    "x_red_wine_30 = red_wine_30.drop('quality', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "x_train_white_10, x_test_white_10, y_train_white_10, y_test_white_10 = train_test_split(x_white_wine_10, y_white_wine_10, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"For white wine quality 10% of data\")\n",
    "\n",
    "print(\"Supervised Learning Algorithms\")\n",
    "runDecisonTreeClassifier(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "runMultiLayerPerceptron(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "runKnnClassifier(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "\n",
    "print(\"Unsupervised Learning Algorithms\")\n",
    "# runKMeans(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "# runDBScan(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "# runAgglomerativeHierarchicalClustering(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "\n",
    "\n",
    "x_train_white_20, x_test_white_20, y_train_white_20, y_test_white_20 = train_test_split(x_white_wine_20, y_white_wine_20, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"For white wine quality 20% of data\")\n",
    "\n",
    "print(\"Supervised Learning Algorithms\")\n",
    "runDecisonTreeClassifier(x_train_white_20, y_train_white_20, x_test_white_20, y_test_white_20)\n",
    "runMultiLayerPerceptron(x_train_white_20, y_train_white_20, x_test_white_20, y_test_white_20)\n",
    "runKnnClassifier(x_train_white_20, y_train_white_20, x_test_white_20, y_test_white_20)\n",
    "\n",
    "print(\"Unsupervised Learning Algorithms\")\n",
    "# runKMeans(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "# runDBScan(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "# runAgglomerativeHierarchicalClustering(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "\n",
    "\n",
    "x_train_white_30, x_test_white_30, y_train_white_30, y_test_white_30 = train_test_split(x_white_wine_30, y_white_wine_30, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"For white wine quality 30% of data\")\n",
    "\n",
    "print(\"Supervised Learning Algorithms\")\n",
    "runDecisonTreeClassifier(x_train_white_30, y_train_white_30, x_test_white_30, y_test_white_30)\n",
    "runMultiLayerPerceptron(x_train_white_30, y_train_white_30, x_test_white_30, y_test_white_30)\n",
    "runKnnClassifier(x_train_white_30, y_train_white_30, x_test_white_30, y_test_white_30)\n",
    "\n",
    "print(\"Unsupervised Learning Algorithms\")\n",
    "# runKMeans(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "# runDBScan(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "# runAgglomerativeHierarchicalClustering(x_train_white_10, y_train_white_10, x_test_white_10, y_test_white_10)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
